<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CYK with Unary Rules]]></title>
    <url>%2F2018%2F12%2F08%2FCYK-with-Unary-Rules%2F</url>
    <content type="text"><![CDATA[Grammar Rules$S$ or $R$ is used as the root node in the following discussion, so $O(S)=O(R)=1$. Binary Rules (w) Unary Rules (w) Lexicons (w) Possible Chain Rules D-&gt;BC (2)E-&gt;BC (3)F-&gt;BC (4)S-&gt;AD (2)S-&gt;AE (3)S-&gt;AF (3)S-&gt;AG (2) F-&gt;E (2)E-&gt;D (3)G-&gt;E (3) A-&gt;w0 (2)B-&gt;w1 (2)C-&gt;w2 (1) nullF-&gt;EG-&gt;EE-&gt;DF-&gt;E-&gt;DG-&gt;E-&gt;D When you draw out all the possible parse trees spanning the sentence w0,w1,w2, you could find the only difference among these parse trees is about the choices of chain rules. We only consider chain unary rules of length less than or equal to 2 in weighted context free grammar, and define 3 levels for inside scores and outside scores of nonterminals involved in such chains. Level index that ranges from 0 to 2 increases from the bottom up (top down) for inside scores (outside scores). Each level L contains nonterminals that are exactly the upmost (bottommost) ones in the chain unary rules of length L for inside scores (outside scores). Inside Scores &amp; Outside Scores Levels Inside Scores (w) Outside Scores 0 I0(D) = w(D-&gt;BC)I(B)I(C)=4I0(E) = w(E-&gt;BC)I(B)I(C)=6I0(F) = w(F-&gt;BC)I(B)I(C)=8 O0(D) = I(A)w(S-&gt;AD)O(S)=4O0(E) = I(A)w(S-&gt;AE)O(S)=6O0(F) = I(A)w(S-&gt;AF)O(S)=6O0(G) = I(A)w(S-&gt;AG)O(S)=4 1 I1(E) = w(E-&gt;D)I0(D)=12I1(F) = w(F-&gt;E)I0(E)=12I1(G) = w(G-&gt;E)I0(E)=18 O1(D) = w(E-&gt;D)O0(E)=18O1(E) = w(F-&gt;E)O0(F) + w(G-&gt;E)O0(G)=24 2 I2(F) = w(F-&gt;E)I1(E)=24I2(G) = w(G-&gt;E)I1(E)=36 O2(D) = w(E-&gt;D)O1(E)=72 From inside scores in the chain unary rules in the above table, we can compute the score of the sentence $w_{0}w_{1}w_{2}$ as \begin{align} I(S) = I(A)\sum_{X \in \mathbf{X}} w(S \rightarrow AX)\sum_{i = 0, 1, 2} I_{i}(X), \end{align} where $S$ is the root node, $\mathbf{X}$ is the nonterminal set, $ w(S \rightarrow AX) = I_{i}(X) = 0 $ if such rules or inside scores do not exist. And \begin{align*} I(S) &= 2 \cdot ((4 \times 2 + 6 \times 3 + 8 \times 3) \\ &+ (12 \times 3 + 12 \times 3 + 18 \times 2) + (24 \times 3 + 36 \times 2)) \\ &= ((16 + 36 + 46) + (72 + 72 + 72) + (144 + 144)) \end{align*} Additional Grammar RulesWe introduce a constant M to represent the weight of the partial parse trees that dominate $ w_{2}w_{3} $ and replace $S$ with $J, K, L$. Also we add the additional grammar rules presented in the following table. The chain unary rules starting with $R$ are not included in possible chain rules. Binary Rules (w) Unary Rules (w) Lexicons (w) Possible Chain Rules J-&gt;AM (2) K-&gt;AM (1) L-&gt;AM (3) K-&gt;J (2)L-&gt;K (3)R-&gt;J (3) R-&gt;K (2) R-&gt;L (1) nullK-&gt;JL-&gt;KL-&gt;K-&gt;J Levels Inside Scores (w) Outside Scores 0 I0(J) = w(J-&gt;AM)I(A)I(M)=4MI0(K) = w(K-&gt;AM)I(A)I(M)=2MI0(L) = w(L-&gt;AM)I(A)I(M)=6M O0(J) = w(R-&gt;J)O(R)=3O0(K) = w(R-&gt;K)O(R)=2O0(L) = w(R-&gt;L)O(R)=1 1 I1(K) = w(K-&gt;J)I0(J)=8MI1(L) = w(L-&gt;K)I0(K)=6M O1(J) = w(K-&gt;J)O0(K)=4O1(K) = w(L-&gt;K)O0(L)=3 2 I2(L) = w(L-&gt;K)I1(K)=24M O2(J) = w(K-&gt;J)O1(K)=6 \begin{align*} I(R) &= ((3 \times 4M + 2 \times 2M + 1 \times 6M) + (2 \times 8M + 1 \times 6M) + (1 \times 24M)) \\ &= ((12M + 4M + 6M) + (16M + 6M) + (24M)) \end{align*} Counts of Binary Rule $c(S \rightarrow AX)$For any nonterminal $X$, we need to find all its occurances in three levels. When $X = D$, \begin{align*} c(S \rightarrow AD) &= I(A) O(S) w(S \rightarrow AD) I(D) \\ &= I(A) w(S \rightarrow AD) \sum_{j = 0, 1, 2}O_{j}(S) \sum_{i = 0, 1, 2} I_{i}(D) \\ \end{align*} \begin{align*} c(J \rightarrow AM) &= w(R \rightarrow J)w(J \rightarrow AM)w(A \rightarrow w_{1})M \\ &+ w(R \rightarrow K)w(K \rightarrow J)w(J \rightarrow AM)w(A \rightarrow w_{1})M \\ &+ w(R \rightarrow L)w(L \rightarrow K)w(K \rightarrow J)w(J \rightarrow AM)w(A \rightarrow w_{1})M \\ &= 12M + 16M + 24M = 52M \\ &= \color{red}{ I(A)w(J \rightarrow AM)O(J)M} \\ &= \color{red}{ I(A)w(J \rightarrow AM)\sum_{i = 0,1,2}O_{i}(J)M} \\ &= 2 \times 2 \times (3 + 4 + 6)M = 52M \end{align*} Counts of Unary Rule $c(X \rightarrow Y)$ \begin{align*} c(X \rightarrow Y) &= \{c(X \rightarrow Y) | X \rightarrow Y \text{ is the chain rule of length 1}\} \\ &+ \{\sum_{Z \in \mathbf{X}} c(X \rightarrow Y \rightarrow Z) + c(Z \rightarrow X \rightarrow Y) | X \rightarrow Y \text{ is involved in the chain rule of length 2}\} \\ &= w(X \rightarrow Y)(O_{0}(X)I_{0}(Y) + O_{0}(X)I_{1}(Y) + O_{1}(X)I_{0}(D)) \end{align*} Let $X = E, Y = D$, we can verify the above formula. The score of all the parse trees containing the unary rule $E \rightarrow D$ is \begin{align*} c(E \rightarrow D) &= \sum_{T \sim w_{0}w_{1}w_{2} \text{ and } E \rightarrow D \in T} w(T) \\ &= w(A \rightarrow w_{0}) w(S \rightarrow AE) w(E \rightarrow D) w(D \rightarrow BC) w(B \rightarrow w_{1}) w(C \rightarrow w_{2}) \\ &+ w(A \rightarrow w_{0}) w(S \rightarrow AE) w(F \rightarrow E) w(E \rightarrow D) w(D \rightarrow BC) w(B \rightarrow w_{1}) w(C \rightarrow w_{2}) \\ &+ w(A \rightarrow w_{0}) w(S \rightarrow AE) w(G \rightarrow E) w(E \rightarrow D) w(D \rightarrow BC) w(B \rightarrow w_{1}) w(C \rightarrow w_{2}) \\ &= 2 \times 3 \times 3 \times 2 \times 2 \times 1 \\ &+ 2 \times 3 \times 2 \times 3 \times 2 \times 2 \times 1 \\ &+ 2 \times 2 \times 3 \times 3 \times 2 \times 2 \times 1 \\ &= 72 + 144 + 144 \\ &= \color{red}{w(E \rightarrow D)(O_{0}(E)I_{0}(D) + O_{1}(E)I_{0}(D))} \\ &= 3 \times (6 \times 4 + 24 \times 4) \\ &= 72 + 288 \end{align*}]]></content>
      <categories>
        <category>Parsing</category>
      </categories>
      <tags>
        <tag>CYK</tag>
        <tag>Toy Grammars</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2017%2F06%2F17%2FIndex%2F</url>
    <content type="text"><![CDATA[Yanpeng Zhao (Yann) The University of EdinburghMy research interest is in structured prediction and latent variable models. Check out my recent work or email me. I am a Ph.D. student in the ILCC at the University of Edinburgh, under the supervision of Prof. Ivan Titov and Mirella Lapata. I am also a member of the Edinburgh NLP. Before moving to Edinburgh, I spent three wonderful years in the SIST at ShanghaiTech University and worked with Prof. Kewei Tu. News First EMNLP paper received review scores (4.5/5, 4.5/5, 4.5/5)“A clear step forward with novel methods and strong improvement.” – Anonymous Reviews First ACL paper received review scores (6/6, 5/6, 5/6)“The first general framework of using latent vectors for grammars.” – Anonymous Reviews Publications Visually Grounded Compound PCFGsYanpeng Zhao and Ivan Titov.EMNLP 2020. [paper], [code], [reviews].Best Paper Honorable Mention [link]. Preprints An Empirical Study of Compound PCFGsYanpeng Zhao.Preprint 2020. [paper], [code]. Unsupervised Transfer of Semantic Role Models from Verbal to Nominal DomainYanpeng Zhao and Ivan Titov.Preprint 2020. [paper], [code]. Before 2018 Gaussian Mixture Latent Vector GrammarsYanpeng Zhao, Liwen Zhang, Kewei Tu.ACL 2018 (oral). [paper], [code], [reviews].Ranked 1st in the Parsing Track. Structured Attentions for Visual Question AnsweringChen Zhu, Yanpeng Zhao, Shuaiyi Huang, Kewei Tu, and Yi Ma.ICCV 2017 (poster). [paper], [code], [reviews]. Learning Bayesian Network Structures Under Incremental Construction CurriculaYanpeng Zhao, Yetian Chen, Kewei Tu, and Jin Tian.Neurocomputing 2017. [paper]. Sequence Prediction Using Neural Network ClassifiersYanpeng Zhao, Shanbo Chu, Yang Zhou, and Kewei Tu.ICGI 2016 (workshop track). [paper], [slides]. Curriculum Learning of Bayesian Network StructuresYanpeng Zhao, Yetian Chen, Kewei Tu, and Jin Tian.ACML 2015 (oral). [paper], [code], [poster], [reviews]. Language Style Transfer from Non-Parallel Text with Arbitrary StylesYanpeng Zhao, Victoria W. Bi, Deng Cai, Xiaojiang Liu, Kewei Tu, Shuming Shi.Technical report, 2018. [paper]. Miscellaneous The avatar photo was taken during my road bike trip around the Qinghai Lake in the summer of 2012]]></content>
  </entry>
  <entry>
    <title><![CDATA[Bayesian Network Structure Learning Code and Data Library]]></title>
    <url>%2F2015%2F08%2F14%2FBayesian-Network-Structure-Learning-Code-And-Data-Library%2F</url>
    <content type="text"><![CDATA[This page contains some software packages of PGM, which were found during my research on BNSL (Bayesian Network Structure Learning). BNSL Source Code LibraryWekaWeka Wiki, Codes of BNSL in Weka lie in weka.classifiers.bayes.net.search. You can refer to to be added about the usage of Weka BNSL. BNTDeveloped by Kevin Murphy, Refer to How to use the Bayes Net Toolbox, Codes of BNSL lie in SLP. By the way, SLP is separately developed based on BNT, and it has been already included in BNT. Another package developed based on BNT is Mateda2.0. Kevin Murphy has also developed BDAGL:Bayesian DAG learning, but I never tried it. Causal ExplorerSource codes of the algorithm described in this paper are not provided (only .p files of the Matlab are available). My implementation of this algorithm in java can be accessed from MMPC. Here is MMHC paper home. JavaBayesI tried it for interchanging file formats of BN. The Interchange Format for Bayesian Networks is to summarize and distribute information on an effort to standardize formats for representation of Bayesian networks and other related graphical models, but it seems to have stalled. BNJIf you wanna get ideas of the implementation of Sparse Candidate, you can look through this one. But the documentation is not that good. E.g., Dev Docs doesn’t give the clear update infos. BanjoSimple and clear, could be used as the reference in developments. OpenMarkovThe project is hosted on Bitbucket. TetradLeaded by Peter Spirtes. Here is the induction from it’s home page …is to develop, analyze, implement, test and apply practical, provably correct computer programs for inferring causal structure under conditions where this is possible. ElviraSome classical algorithms such as PC, K2 are included. PGM ToolboxIt perfectly explains OOP (Object Oriented Programming) in matlab. Infer &amp; URLearning &amp; WinMineI have never tried them. Here are some other summaries of software packages of PGM: Graphical Models Software Tools, Bayes Nets. Bayesian Network RepositoryBNLearn and Software Packages for Graphical Models and GalElidan.]]></content>
      <categories>
        <category>Manual</category>
      </categories>
      <tags>
        <tag>Bayesian Network</tag>
        <tag>Structure Learning</tag>
        <tag>Codes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Math Support for Hexo]]></title>
    <url>%2F2015%2F04%2F11%2FMath-Support-for-Hexo%2F</url>
    <content type="text"><![CDATA[_ in Markdown conflicts with that in Latex, here is the solution to it: Hexo uses Nunjucks to render posts (Swig was used in older version, which share a similar syntax). Content wrapped with {{ }} or {% %} will get parsed and may cause problems. You can wrap sensitive content with the raw tag plugin. 12345&#123;% raw %&#125;$$e^&#123;i \theta_&#123;t&#125;&#125; = \cos \theta_&#123;t&#125; + i \sin \theta_&#123;t&#125;$$&#123;% endraw %&#125; Results $$ e^{i \theta_{t}} = \cos \theta_{t} + i \sin \theta_{t} $$ Ref: https://hexo.io/docs/troubleshooting.html#Escape-Contents]]></content>
      <categories>
        <category>Manual</category>
      </categories>
      <tags>
        <tag>mathjax</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Beautiful Equation]]></title>
    <url>%2F2015%2F04%2F04%2FA-Beautiful-Equation%2F</url>
    <content type="text"><![CDATA[$${e \approx \left(1 + 9^{-4^{7 \times 6}}\right)^{3^{2^{85}}}}$$]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>equation</tag>
        <tag>exp</tag>
      </tags>
  </entry>
</search>
